{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedViT Attention Map Visualization\n",
    "\n",
    "This notebook demonstrates how to visualize attention maps from trained MedViT models.\n",
    "\n",
    "MedViT uses two types of attention mechanisms:\n",
    "1. **LFP (Local Feature Processing)**: Neighborhood Attention or Standard Multi-Head Attention\n",
    "2. **GFP (Global Feature Processing)**: E-MHSA (Efficient Multi-Head Self Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from visualize_attention import AttentionVisualizer, load_model\n",
    "\n",
    "# Auto-reload modules for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = 'MedViT_tiny'\n",
    "CHECKPOINT_PATH = '../checkpoint/MedViT_tiny_brain_tumor.pth'  # Update this path\n",
    "NUM_CLASSES = 4  # Brain tumor dataset: glioma, meningioma, pituitary, no-tumor\n",
    "\n",
    "# Class names for brain tumor dataset\n",
    "CLASS_NAMES = ['glioma', 'meningioma', 'no-tumor', 'pituitary']\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = load_model(\n",
    "    model_name=MODEL_NAME,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Attention Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the visualizer\n",
    "visualizer = AttentionVisualizer(model, device)\n",
    "\n",
    "print(\"Visualizer created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Sample Image from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the brain tumor dataset\n",
    "dataset = load_dataset('PranomVignesh/MRI-Images-of-Brain-Tumor', split='test')\n",
    "\n",
    "# Get a sample image\n",
    "sample_idx = 0  # Change this to view different samples\n",
    "sample = dataset[sample_idx]\n",
    "sample_image = sample['image']\n",
    "sample_label = sample['label']\n",
    "\n",
    "print(f\"Sample {sample_idx}\")\n",
    "print(f\"Label: {sample_label} ({CLASS_NAMES[sample_label]})\")\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(sample_image)\n",
    "plt.title(f\"Original Image\\nLabel: {CLASS_NAMES[sample_label]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Attention Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sample image temporarily for visualization\n",
    "temp_image_path = '/tmp/sample_brain_mri.png'\n",
    "sample_image.save(temp_image_path)\n",
    "\n",
    "# Visualize attention maps for all layers\n",
    "attention_maps, pred_class = visualizer.visualize(\n",
    "    image_path=temp_image_path,\n",
    "    output_path=None,  # Set to a path to save the figure\n",
    "    show=True,\n",
    "    cmap='jet',\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "print(f\"\\nPredicted class: {pred_class} ({CLASS_NAMES[pred_class]})\")\n",
    "print(f\"True label: {sample_label} ({CLASS_NAMES[sample_label]})\")\n",
    "print(f\"\\nCaptured {len(attention_maps)} attention layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Individual Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available attention layers\n",
    "print(\"Available attention layers:\")\n",
    "for i, name in enumerate(attention_maps.keys()):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all heads for a specific layer\n",
    "# Choose a layer from the list above\n",
    "layer_to_visualize = list(attention_maps.keys())[0]  # First layer\n",
    "\n",
    "visualizer.visualize_all_heads(\n",
    "    image_path=temp_image_path,\n",
    "    layer_name=layer_to_visualize,\n",
    "    show=True,\n",
    "    cmap='jet',\n",
    "    alpha=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Multiple Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataset, idx, visualizer, class_names):\n",
    "    \"\"\"Visualize attention for a specific sample.\"\"\"\n",
    "    sample = dataset[idx]\n",
    "    sample_image = sample['image']\n",
    "    sample_label = sample['label']\n",
    "    \n",
    "    # Save temp image\n",
    "    temp_path = f'/tmp/sample_{idx}.png'\n",
    "    sample_image.save(temp_path)\n",
    "    \n",
    "    # Get attention maps\n",
    "    attention_maps, img_np, pred_class = visualizer.get_attention_maps(temp_path)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, min(4, len(attention_maps) + 1), figsize=(16, 4))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(img_np)\n",
    "    correct = \"✓\" if pred_class == sample_label else \"✗\"\n",
    "    axes[0].set_title(f'Original\\nTrue: {class_names[sample_label]}\\nPred: {class_names[pred_class]} {correct}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Attention maps\n",
    "    for i, (name, attn) in enumerate(list(attention_maps.items())[:3]):\n",
    "        attn_map = visualizer.aggregate_attention(attn, method='mean')\n",
    "        attn_2d = visualizer.reshape_attention_to_image(attn_map, (224, 224))\n",
    "        attn_2d = (attn_2d - attn_2d.min()) / (attn_2d.max() - attn_2d.min() + 1e-8)\n",
    "        \n",
    "        axes[i+1].imshow(img_np)\n",
    "        axes[i+1].imshow(attn_2d, cmap='jet', alpha=0.5)\n",
    "        short_name = name.split('.')[-1]\n",
    "        axes[i+1].set_title(f'Layer {i}: {short_name}')\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few samples from each class\n",
    "for class_idx in range(NUM_CLASSES):\n",
    "    # Find a sample of this class\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i]['label'] == class_idx:\n",
    "            print(f\"\\n=== Class: {CLASS_NAMES[class_idx]} ===\")\n",
    "            visualize_sample(dataset, i, visualizer, CLASS_NAMES)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention maps for the current sample\n",
    "attention_maps, img_np, pred_class = visualizer.get_attention_maps(temp_image_path)\n",
    "\n",
    "# Analyze attention statistics\n",
    "print(\"Attention Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "for name, attn in attention_maps.items():\n",
    "    print(f\"\\nLayer: {name}\")\n",
    "    print(f\"  Shape: {attn.shape}\")\n",
    "    print(f\"  Min: {attn.min().item():.4f}\")\n",
    "    print(f\"  Max: {attn.max().item():.4f}\")\n",
    "    print(f\"  Mean: {attn.mean().item():.4f}\")\n",
    "    print(f\"  Std: {attn.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention distribution\n",
    "fig, axes = plt.subplots(1, len(attention_maps), figsize=(4 * len(attention_maps), 4))\n",
    "if len(attention_maps) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (name, attn) in zip(axes, attention_maps.items()):\n",
    "    # Flatten and plot histogram\n",
    "    attn_flat = attn.flatten().numpy()\n",
    "    ax.hist(attn_flat, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(f'{name.split(\".\")[-1]}')\n",
    "    ax.set_xlabel('Attention Weight')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Attention Weight Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
